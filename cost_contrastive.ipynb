{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cost_contrastive.ipynb",
      "provenance": [],
      "mount_file_id": "171hsAzo3pwB0B1ItH5Wcc7fTHPoFrhTT",
      "authorship_tag": "ABX9TyNpMLnz8A5oGAVVvpb7EyID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HrudayR/CoST/blob/master/cost_contrastive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tCfNbDEVrKD",
        "outputId": "3f923a8f-d97c-4dbe-e335-94edc878e496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO2X1y_-V_yB",
        "outputId": "39ec9b9a-4feb-4338-dcb0-1144fe9b7e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Colab\\ Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkebVotrWDyh",
        "outputId": "51693681-9f41-4555-f431-6be6c0e34c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Cost_contrastive_learning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8raykp4bWJ9B",
        "outputId": "7b5c68e2-b568-47e5-d48d-e471c63db079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Cost_contrastive_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CoST-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joZ7zt2LWSFw",
        "outputId": "aebfce8a-0be6-4712-d71c-c9c32baede38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Cost_contrastive_learning/CoST-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BzvhjIIkYCUj",
        "outputId": "e2840019-98aa-4c4d-ff00-f604d9863ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.6.1\n",
            "  Downloading scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 49.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.8 kB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 39.0 MB/s \n",
            "\u001b[?25hCollecting pandas==1.0.1\n",
            "  Downloading pandas-1.0.1-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 27.6 MB/s \n",
            "\u001b[?25hCollecting scikit_learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 61.0 MB/s \n",
            "\u001b[?25hCollecting einops==0.3.0\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.1->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.1->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.1->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.1->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.1->-r requirements.txt (line 4)) (1.15.0)\n",
            "Installing collected packages: numpy, scipy, torch, scikit-learn, pandas, einops\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.1 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.0 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 1.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed einops-0.3.0 numpy-1.19.2 pandas-1.0.1 scikit-learn-0.24.1 scipy-1.6.1 torch-1.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzoDwtf3cxty",
        "outputId": "5466beea-dc35-4180-d35b-1b5c29d6d217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] --archive ARCHIVE [--gpu GPU] [--batch-size BATCH_SIZE]\n",
            "                [--lr LR] [--repr-dims REPR_DIMS]\n",
            "                [--max-train-length MAX_TRAIN_LENGTH] [--iters ITERS]\n",
            "                [--epochs EPOCHS] [--save-every SAVE_EVERY] [--seed SEED]\n",
            "                [--max-threads MAX_THREADS] [--eval]\n",
            "                [--kernels KERNELS [KERNELS ...]] [--alpha ALPHA]\n",
            "                dataset run_name\n",
            "\n",
            "positional arguments:\n",
            "  dataset               The dataset name\n",
            "  run_name              The folder name used to save model, output and\n",
            "                        evaluation metrics. This can be set to any word\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --archive ARCHIVE     The archive name that the dataset belongs to. This can\n",
            "                        be set to forecast_csv, or forecast_csv_univar\n",
            "  --gpu GPU             The gpu no. used for training and inference (defaults\n",
            "                        to 0)\n",
            "  --batch-size BATCH_SIZE\n",
            "                        The batch size (defaults to 8)\n",
            "  --lr LR               The learning rate (defaults to 0.001)\n",
            "  --repr-dims REPR_DIMS\n",
            "                        The representation dimension (defaults to 320)\n",
            "  --max-train-length MAX_TRAIN_LENGTH\n",
            "                        For sequence with a length greater than\n",
            "                        <max_train_length>, it would be cropped into some\n",
            "                        sequences, each of which has a length less than\n",
            "                        <max_train_length> (defaults to 3000)\n",
            "  --iters ITERS         The number of iterations\n",
            "  --epochs EPOCHS       The number of epochs\n",
            "  --save-every SAVE_EVERY\n",
            "                        Save the checkpoint every <save_every>\n",
            "                        iterations/epochs\n",
            "  --seed SEED           The random seed\n",
            "  --max-threads MAX_THREADS\n",
            "                        The maximum allowed number of threads used by this\n",
            "                        process\n",
            "  --eval                Whether to perform evaluation after training\n",
            "  --kernels KERNELS [KERNELS ...]\n",
            "  --alpha ALPHA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -u train.py ETTh1 forecast_multivar --alpha 0.0005 --kernels 1 2 4 8 16 32 64 128 --max-train-length 201 --batch-size 128 --archive forecast_csv_univar --repr-dims 320 --max-threads 8 --eval --epochs 100"
      ],
      "metadata": {
        "id": "n2MFpKy2Y2K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u train.py dataset4 forecast_dataset4 --alpha 0.0005 --kernels 1 2 4 8 16 32 64 128 --max-train-length 201 --batch-size 128 --archive forecast_csv_univar --repr-dims 320 --max-threads 8 --eval --epochs 300"
      ],
      "metadata": {
        "id": "CDJxE5zRZj7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e62ee0-3f43-43a2-915a-6ce5bb22a652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: dataset4\n",
            "Arguments: Namespace(alpha=0.0005, archive='forecast_csv_univar', batch_size=128, dataset='dataset4', epochs=300, eval=True, gpu=0, iters=None, kernels=[1, 2, 4, 8, 16, 32, 64, 128], lr=0.001, max_threads=8, max_train_length=201, repr_dims=320, run_name='forecast_dataset4', save_every=None, seed=None)\n",
            "Epoch #0: loss=0.06432940810918808\n",
            "Epoch #1: loss=4.431066989898682\n",
            "Epoch #2: loss=5.158374786376953\n",
            "Epoch #3: loss=5.072782516479492\n",
            "Epoch #4: loss=5.048680305480957\n",
            "Epoch #5: loss=5.003917217254639\n",
            "Epoch #6: loss=4.8990583419799805\n",
            "Epoch #7: loss=4.878839492797852\n",
            "Epoch #8: loss=4.716128349304199\n",
            "Epoch #9: loss=4.765833377838135\n",
            "Epoch #10: loss=4.740506649017334\n",
            "Epoch #11: loss=4.65007209777832\n",
            "Epoch #12: loss=4.852314472198486\n",
            "Epoch #13: loss=4.723496437072754\n",
            "Epoch #14: loss=4.8498992919921875\n",
            "Epoch #15: loss=4.530856132507324\n",
            "Epoch #16: loss=4.717922210693359\n",
            "Epoch #17: loss=4.626073837280273\n",
            "Epoch #18: loss=4.623666763305664\n",
            "Epoch #19: loss=4.4325337409973145\n",
            "Epoch #20: loss=4.646986484527588\n",
            "Epoch #21: loss=4.442883491516113\n",
            "Epoch #22: loss=4.622572422027588\n",
            "Epoch #23: loss=4.546374797821045\n",
            "Epoch #24: loss=4.627476215362549\n",
            "Epoch #25: loss=4.311031341552734\n",
            "Epoch #26: loss=4.576346397399902\n",
            "Epoch #27: loss=4.4904866218566895\n",
            "Epoch #28: loss=4.3119916915893555\n",
            "Epoch #29: loss=4.471827507019043\n",
            "Epoch #30: loss=4.259511470794678\n",
            "Epoch #31: loss=4.075199604034424\n",
            "Epoch #32: loss=4.421638011932373\n",
            "Epoch #33: loss=4.469795227050781\n",
            "Epoch #34: loss=4.230510711669922\n",
            "Epoch #35: loss=4.169569492340088\n",
            "Epoch #36: loss=4.3113274574279785\n",
            "Epoch #37: loss=4.119056224822998\n",
            "Epoch #38: loss=4.350728988647461\n",
            "Epoch #39: loss=4.364680290222168\n",
            "Epoch #40: loss=4.349148273468018\n",
            "Epoch #41: loss=4.179398536682129\n",
            "Epoch #42: loss=4.097634792327881\n",
            "Epoch #43: loss=4.200751304626465\n",
            "Epoch #44: loss=3.997459888458252\n",
            "Epoch #45: loss=4.220085144042969\n",
            "Epoch #46: loss=4.274107933044434\n",
            "Epoch #47: loss=4.124521255493164\n",
            "Epoch #48: loss=3.83964204788208\n",
            "Epoch #49: loss=4.179069995880127\n",
            "Epoch #50: loss=4.006900310516357\n",
            "Epoch #51: loss=4.402028560638428\n",
            "Epoch #52: loss=4.140002727508545\n",
            "Epoch #53: loss=4.077943801879883\n",
            "Epoch #54: loss=3.9858217239379883\n",
            "Epoch #55: loss=4.078769683837891\n",
            "Epoch #56: loss=3.9519236087799072\n",
            "Epoch #57: loss=3.8763155937194824\n",
            "Epoch #58: loss=3.961038589477539\n",
            "Epoch #59: loss=4.179244518280029\n",
            "Epoch #60: loss=3.9939372539520264\n",
            "Epoch #61: loss=4.234245300292969\n",
            "Epoch #62: loss=4.141996383666992\n",
            "Epoch #63: loss=3.830808639526367\n",
            "Epoch #64: loss=3.8686981201171875\n",
            "Epoch #65: loss=3.9426229000091553\n",
            "Epoch #66: loss=4.082640647888184\n",
            "Epoch #67: loss=3.814023017883301\n",
            "Epoch #68: loss=4.052762985229492\n",
            "Epoch #69: loss=3.8808000087738037\n",
            "Epoch #70: loss=3.392408609390259\n",
            "Epoch #71: loss=3.6941978931427\n",
            "Epoch #72: loss=3.9259071350097656\n",
            "Epoch #73: loss=3.7949137687683105\n",
            "Epoch #74: loss=3.471914768218994\n",
            "Epoch #75: loss=3.9876222610473633\n",
            "Epoch #76: loss=3.7269930839538574\n",
            "Epoch #77: loss=3.8382370471954346\n",
            "Epoch #78: loss=3.5388786792755127\n",
            "Epoch #79: loss=3.9314961433410645\n",
            "Epoch #80: loss=3.821171522140503\n",
            "Epoch #81: loss=3.7159979343414307\n",
            "Epoch #82: loss=3.4761836528778076\n",
            "Epoch #83: loss=3.772914409637451\n",
            "Epoch #84: loss=3.5235888957977295\n",
            "Epoch #85: loss=3.916388511657715\n",
            "Epoch #86: loss=3.672344207763672\n",
            "Epoch #87: loss=3.3931539058685303\n",
            "Epoch #88: loss=3.8208506107330322\n",
            "Epoch #89: loss=3.710531711578369\n",
            "Epoch #90: loss=4.032760143280029\n",
            "Epoch #91: loss=3.0392158031463623\n",
            "Epoch #92: loss=3.8867199420928955\n",
            "Epoch #93: loss=3.780102491378784\n",
            "Epoch #94: loss=3.5570383071899414\n",
            "Epoch #95: loss=3.478891372680664\n",
            "Epoch #96: loss=3.8094849586486816\n",
            "Epoch #97: loss=4.074872016906738\n",
            "Epoch #98: loss=3.9033169746398926\n",
            "Epoch #99: loss=3.734829902648926\n",
            "Epoch #100: loss=3.4926400184631348\n",
            "Epoch #101: loss=3.690518379211426\n",
            "Epoch #102: loss=3.494400978088379\n",
            "Epoch #103: loss=4.010162353515625\n",
            "Epoch #104: loss=3.517258644104004\n",
            "Epoch #105: loss=3.583807945251465\n",
            "Epoch #106: loss=3.613466262817383\n",
            "Epoch #107: loss=3.804856538772583\n",
            "Epoch #108: loss=3.2914516925811768\n",
            "Epoch #109: loss=3.7148404121398926\n",
            "Epoch #110: loss=3.460261583328247\n",
            "Epoch #111: loss=3.3328158855438232\n",
            "Epoch #112: loss=3.7850329875946045\n",
            "Epoch #113: loss=3.7936272621154785\n",
            "Epoch #114: loss=3.6316986083984375\n",
            "Epoch #115: loss=4.246741771697998\n",
            "Epoch #116: loss=3.308128833770752\n",
            "Epoch #117: loss=3.9387741088867188\n",
            "Epoch #118: loss=4.186645030975342\n",
            "Epoch #119: loss=3.329958915710449\n",
            "Epoch #120: loss=3.848461627960205\n",
            "Epoch #121: loss=3.322225332260132\n",
            "Epoch #122: loss=3.6366403102874756\n",
            "Epoch #123: loss=3.4901349544525146\n",
            "Epoch #124: loss=4.042154788970947\n",
            "Epoch #125: loss=4.139162540435791\n",
            "Epoch #126: loss=2.988938093185425\n",
            "Epoch #127: loss=3.2288081645965576\n",
            "Epoch #128: loss=3.8011598587036133\n",
            "Epoch #129: loss=3.616959571838379\n",
            "Epoch #130: loss=3.409014940261841\n",
            "Epoch #131: loss=3.1635982990264893\n",
            "Epoch #132: loss=3.108224630355835\n",
            "Epoch #133: loss=3.4551126956939697\n",
            "Epoch #134: loss=3.50421404838562\n",
            "Epoch #135: loss=3.7361347675323486\n",
            "Epoch #136: loss=3.64888334274292\n",
            "Epoch #137: loss=3.372555732727051\n",
            "Epoch #138: loss=3.5100133419036865\n",
            "Epoch #139: loss=3.4205873012542725\n",
            "Epoch #140: loss=3.1166770458221436\n",
            "Epoch #141: loss=3.4315061569213867\n",
            "Epoch #142: loss=3.3092281818389893\n",
            "Epoch #143: loss=3.3114211559295654\n",
            "Epoch #144: loss=2.6463630199432373\n",
            "Epoch #145: loss=3.5995562076568604\n",
            "Epoch #146: loss=3.4738311767578125\n",
            "Epoch #147: loss=3.5096824169158936\n",
            "Epoch #148: loss=3.3403987884521484\n",
            "Epoch #149: loss=3.3458409309387207\n",
            "Epoch #150: loss=3.141376495361328\n",
            "Epoch #151: loss=3.8240392208099365\n",
            "Epoch #152: loss=2.736528158187866\n",
            "Epoch #153: loss=3.280864715576172\n",
            "Epoch #154: loss=3.5782296657562256\n",
            "Epoch #155: loss=3.9872543811798096\n",
            "Epoch #156: loss=3.523406744003296\n",
            "Epoch #157: loss=3.6495521068573\n",
            "Epoch #158: loss=3.075895071029663\n",
            "Epoch #159: loss=3.851611852645874\n",
            "Epoch #160: loss=3.1369776725769043\n",
            "Epoch #161: loss=3.042325496673584\n",
            "Epoch #162: loss=3.000443696975708\n",
            "Epoch #163: loss=2.8446993827819824\n",
            "Epoch #164: loss=3.405651569366455\n",
            "Epoch #165: loss=3.2771077156066895\n",
            "Epoch #166: loss=3.080737590789795\n",
            "Epoch #167: loss=3.487656593322754\n",
            "Epoch #168: loss=3.3252480030059814\n",
            "Epoch #169: loss=3.4316210746765137\n",
            "Epoch #170: loss=4.025929927825928\n",
            "Epoch #171: loss=3.1987814903259277\n",
            "Epoch #172: loss=3.123365879058838\n",
            "Epoch #173: loss=2.937814712524414\n",
            "Epoch #174: loss=3.80264949798584\n",
            "Epoch #175: loss=3.2812154293060303\n",
            "Epoch #176: loss=3.475904703140259\n",
            "Epoch #177: loss=2.849993944168091\n",
            "Epoch #178: loss=3.406569719314575\n",
            "Epoch #179: loss=3.5621650218963623\n",
            "Epoch #180: loss=2.65352463722229\n",
            "Epoch #181: loss=3.3555378913879395\n",
            "Epoch #182: loss=3.128234386444092\n",
            "Epoch #183: loss=3.3341803550720215\n",
            "Epoch #184: loss=3.1201839447021484\n",
            "Epoch #185: loss=3.088949680328369\n",
            "Epoch #186: loss=3.3581206798553467\n",
            "Epoch #187: loss=2.92028546333313\n",
            "Epoch #188: loss=2.951800584793091\n",
            "Epoch #189: loss=2.8195228576660156\n",
            "Epoch #190: loss=2.78670597076416\n",
            "Epoch #191: loss=2.8982133865356445\n",
            "Epoch #192: loss=4.005156993865967\n",
            "Epoch #193: loss=3.7873520851135254\n",
            "Epoch #194: loss=2.444023609161377\n",
            "Epoch #195: loss=2.7026023864746094\n",
            "Epoch #196: loss=3.7254536151885986\n",
            "Epoch #197: loss=2.9957985877990723\n",
            "Epoch #198: loss=2.7059743404388428\n",
            "Epoch #199: loss=3.4295952320098877\n",
            "Epoch #200: loss=2.8957200050354004\n",
            "Epoch #201: loss=2.8268277645111084\n",
            "Epoch #202: loss=3.749532461166382\n",
            "Epoch #203: loss=3.761406421661377\n",
            "Epoch #204: loss=2.9455065727233887\n",
            "Epoch #205: loss=3.555933713912964\n",
            "Epoch #206: loss=2.4661049842834473\n",
            "Epoch #207: loss=2.786012649536133\n",
            "Epoch #208: loss=3.2874410152435303\n",
            "Epoch #209: loss=3.0567450523376465\n",
            "Epoch #210: loss=3.258333444595337\n",
            "Epoch #211: loss=2.7583556175231934\n",
            "Epoch #212: loss=2.7409510612487793\n",
            "Epoch #213: loss=2.89958119392395\n",
            "Epoch #214: loss=2.4477508068084717\n",
            "Epoch #215: loss=3.651432991027832\n",
            "Epoch #216: loss=3.318726062774658\n",
            "Epoch #217: loss=3.3184988498687744\n",
            "Epoch #218: loss=3.1252453327178955\n",
            "Epoch #219: loss=3.336526393890381\n",
            "Epoch #220: loss=2.788383722305298\n",
            "Epoch #221: loss=3.2538115978240967\n",
            "Epoch #222: loss=3.309021234512329\n",
            "Epoch #223: loss=2.619917154312134\n",
            "Epoch #224: loss=2.5483031272888184\n",
            "Epoch #225: loss=2.4048290252685547\n",
            "Epoch #226: loss=2.5133895874023438\n",
            "Epoch #227: loss=3.4304141998291016\n",
            "Epoch #228: loss=2.7399368286132812\n",
            "Epoch #229: loss=2.9835143089294434\n",
            "Epoch #230: loss=3.1165661811828613\n",
            "Epoch #231: loss=2.9849467277526855\n",
            "Epoch #232: loss=3.264850616455078\n",
            "Epoch #233: loss=2.516913890838623\n",
            "Epoch #234: loss=2.823380470275879\n",
            "Epoch #235: loss=2.8954505920410156\n",
            "Epoch #236: loss=2.4792325496673584\n",
            "Epoch #237: loss=2.3987350463867188\n",
            "Epoch #238: loss=3.127183675765991\n",
            "Epoch #239: loss=2.632268190383911\n",
            "Epoch #240: loss=3.0669569969177246\n",
            "Epoch #241: loss=1.8516290187835693\n",
            "Epoch #242: loss=2.991549491882324\n",
            "Epoch #243: loss=3.178392171859741\n",
            "Epoch #244: loss=3.328622579574585\n",
            "Epoch #245: loss=2.0391955375671387\n",
            "Epoch #246: loss=2.8153703212738037\n",
            "Epoch #247: loss=3.449674606323242\n",
            "Epoch #248: loss=3.136504650115967\n",
            "Epoch #249: loss=3.118980646133423\n",
            "Epoch #250: loss=2.5051512718200684\n",
            "Epoch #251: loss=2.796693801879883\n",
            "Epoch #252: loss=2.9569787979125977\n",
            "Epoch #253: loss=2.791574478149414\n",
            "Epoch #254: loss=2.11430025100708\n",
            "Epoch #255: loss=3.310539722442627\n",
            "Epoch #256: loss=3.3596792221069336\n",
            "Epoch #257: loss=2.8752200603485107\n",
            "Epoch #258: loss=3.233496904373169\n",
            "Epoch #259: loss=2.868997812271118\n",
            "Epoch #260: loss=2.330258846282959\n",
            "Epoch #261: loss=2.7930748462677\n",
            "Epoch #262: loss=2.7289011478424072\n",
            "Epoch #263: loss=3.445310115814209\n",
            "Epoch #264: loss=3.394726276397705\n",
            "Epoch #265: loss=2.884411573410034\n",
            "Epoch #266: loss=2.9751291275024414\n",
            "Epoch #267: loss=3.4504566192626953\n",
            "Epoch #268: loss=3.284942388534546\n",
            "Epoch #269: loss=2.736232280731201\n",
            "Epoch #270: loss=2.4050629138946533\n",
            "Epoch #271: loss=2.9078242778778076\n",
            "Epoch #272: loss=2.814976692199707\n",
            "Epoch #273: loss=3.422844886779785\n",
            "Epoch #274: loss=2.8490734100341797\n",
            "Epoch #275: loss=2.3612418174743652\n",
            "Epoch #276: loss=2.5580060482025146\n",
            "Epoch #277: loss=3.3563992977142334\n",
            "Epoch #278: loss=3.070343255996704\n",
            "Epoch #279: loss=2.4532382488250732\n",
            "Epoch #280: loss=2.348097085952759\n",
            "Epoch #281: loss=3.215519905090332\n",
            "Epoch #282: loss=2.542065382003784\n",
            "Epoch #283: loss=3.3579883575439453\n",
            "Epoch #284: loss=3.1776785850524902\n",
            "Epoch #285: loss=2.933436870574951\n",
            "Epoch #286: loss=2.4428293704986572\n",
            "Epoch #287: loss=3.8971879482269287\n",
            "Epoch #288: loss=2.6084165573120117\n",
            "Epoch #289: loss=3.0124449729919434\n",
            "Epoch #290: loss=2.2271852493286133\n",
            "Epoch #291: loss=2.391977548599243\n",
            "Epoch #292: loss=2.914945125579834\n",
            "Epoch #293: loss=3.341142416000366\n",
            "Epoch #294: loss=1.948437213897705\n",
            "Epoch #295: loss=2.5660195350646973\n",
            "Epoch #296: loss=3.31997013092041\n",
            "Epoch #297: loss=2.37024188041687\n",
            "Epoch #298: loss=2.0079894065856934\n",
            "Epoch #299: loss=2.550694227218628\n",
            "\n",
            "Training time: 0:08:18.417037\n",
            "\n",
            "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.33686220628439056, 'MAE': 0.43069513466158466}, 'raw': {'MSE': 0.06486189621158148, 'MAE': 0.18898992021001346}}, 48: {'norm': {'MSE': 0.3576686906473619, 'MAE': 0.46176615031059576}, 'raw': {'MSE': 0.06886812781092766, 'MAE': 0.20262394764071393}}, 96: {'norm': {'MSE': 0.4463079580673423, 'MAE': 0.5240190135045875}, 'raw': {'MSE': 0.08593537557076275, 'MAE': 0.22994063138027823}}, 168: {'norm': {'MSE': 2.467374541554386, 'MAE': 1.2618150701601758}, 'raw': {'MSE': 0.47508621205677215, 'MAE': 0.553687073127338}}, 336: {'norm': {'MSE': 17.407831429078566, 'MAE': 3.2262110559792982}, 'raw': {'MSE': 3.3518302186204565, 'MAE': 1.415668097594863}}}, 'encoder_infer_time': 4.9210638999938965, 'lr_train_time': {24: 0.3003885746002197, 48: 0.40131330490112305, 96: 0.3360757827758789, 168: 0.41759657859802246, 336: 0.5029866695404053}, 'lr_infer_time': {24: 0.0008184909820556641, 48: 0.0010349750518798828, 96: 0.0033447742462158203, 168: 0.0034339427947998047, 336: 0.0020904541015625}}\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u train.py mendeley forecast_mendeley --alpha 0.0005 --kernels 1 2 4 8 16 32 64 128 --max-train-length 201 --batch-size 128 --archive forecast_csv_univar --repr-dims 320 --max-threads 8 --eval --epochs 300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkmPxnnKIkIg",
        "outputId": "4499cb75-c026-4c52-90e4-3301d1f16230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: mendeley\n",
            "Arguments: Namespace(alpha=0.0005, archive='forecast_csv_univar', batch_size=128, dataset='mendeley', epochs=300, eval=True, gpu=0, iters=None, kernels=[1, 2, 4, 8, 16, 32, 64, 128], lr=0.001, max_threads=8, max_train_length=201, repr_dims=320, run_name='forecast_mendeley', save_every=None, seed=None)\n",
            "Epoch #0: loss=0.0739838182926178\n",
            "Epoch #1: loss=4.331783771514893\n",
            "Epoch #2: loss=4.966609954833984\n",
            "Epoch #3: loss=4.811729431152344\n",
            "Epoch #4: loss=4.871388912200928\n",
            "Epoch #5: loss=4.893041610717773\n",
            "Epoch #6: loss=4.81461238861084\n",
            "Epoch #7: loss=4.872995376586914\n",
            "Epoch #8: loss=4.708375930786133\n",
            "Epoch #9: loss=4.733824253082275\n",
            "Epoch #10: loss=4.58901834487915\n",
            "Epoch #11: loss=4.694998264312744\n",
            "Epoch #12: loss=4.609711170196533\n",
            "Epoch #13: loss=4.666107654571533\n",
            "Epoch #14: loss=4.5058274269104\n",
            "Epoch #15: loss=4.447852611541748\n",
            "Epoch #16: loss=4.889238357543945\n",
            "Epoch #17: loss=4.581756114959717\n",
            "Epoch #18: loss=4.540969371795654\n",
            "Epoch #19: loss=4.494584560394287\n",
            "Epoch #20: loss=4.737042427062988\n",
            "Epoch #21: loss=4.60715389251709\n",
            "Epoch #22: loss=4.636767864227295\n",
            "Epoch #23: loss=4.4396467208862305\n",
            "Epoch #24: loss=4.490409851074219\n",
            "Epoch #25: loss=4.585463523864746\n",
            "Epoch #26: loss=4.597152233123779\n",
            "Epoch #27: loss=4.562451362609863\n",
            "Epoch #28: loss=4.5941596031188965\n",
            "Epoch #29: loss=4.1384124755859375\n",
            "Epoch #30: loss=4.371037483215332\n",
            "Epoch #31: loss=4.43889856338501\n",
            "Epoch #32: loss=4.5576677322387695\n",
            "Epoch #33: loss=4.371048927307129\n",
            "Epoch #34: loss=4.535124778747559\n",
            "Epoch #35: loss=4.307445526123047\n",
            "Epoch #36: loss=4.255364418029785\n",
            "Epoch #37: loss=4.396718502044678\n",
            "Epoch #38: loss=4.470571994781494\n",
            "Epoch #39: loss=4.305659294128418\n",
            "Epoch #40: loss=4.383998394012451\n",
            "Epoch #41: loss=4.199405193328857\n",
            "Epoch #42: loss=4.557590961456299\n",
            "Epoch #43: loss=4.543453216552734\n",
            "Epoch #44: loss=4.3205342292785645\n",
            "Epoch #45: loss=4.326396465301514\n",
            "Epoch #46: loss=4.478344917297363\n",
            "Epoch #47: loss=4.740549087524414\n",
            "Epoch #48: loss=4.2132110595703125\n",
            "Epoch #49: loss=4.292866230010986\n",
            "Epoch #50: loss=4.191412925720215\n",
            "Epoch #51: loss=4.395927429199219\n",
            "Epoch #52: loss=4.45297384262085\n",
            "Epoch #53: loss=4.018802642822266\n",
            "Epoch #54: loss=4.0226664543151855\n",
            "Epoch #55: loss=4.263827800750732\n",
            "Epoch #56: loss=4.279055595397949\n",
            "Epoch #57: loss=4.261440753936768\n",
            "Epoch #58: loss=4.221427917480469\n",
            "Epoch #59: loss=4.208072662353516\n",
            "Epoch #60: loss=3.9811642169952393\n",
            "Epoch #61: loss=4.244466304779053\n",
            "Epoch #62: loss=4.219855785369873\n",
            "Epoch #63: loss=4.002781867980957\n",
            "Epoch #64: loss=4.315675258636475\n",
            "Epoch #65: loss=3.8437347412109375\n",
            "Epoch #66: loss=4.116967678070068\n",
            "Epoch #67: loss=4.0714216232299805\n",
            "Epoch #68: loss=3.9303536415100098\n",
            "Epoch #69: loss=4.076445579528809\n",
            "Epoch #70: loss=4.094790935516357\n",
            "Epoch #71: loss=3.9609196186065674\n",
            "Epoch #72: loss=4.659677028656006\n",
            "Epoch #73: loss=3.3809943199157715\n",
            "Epoch #74: loss=4.277770042419434\n",
            "Epoch #75: loss=3.794635534286499\n",
            "Epoch #76: loss=4.132917881011963\n",
            "Epoch #77: loss=4.369654655456543\n",
            "Epoch #78: loss=3.7175722122192383\n",
            "Epoch #79: loss=4.042904376983643\n",
            "Epoch #80: loss=4.183462142944336\n",
            "Epoch #81: loss=4.156257629394531\n",
            "Epoch #82: loss=4.042813301086426\n",
            "Epoch #83: loss=4.099521636962891\n",
            "Epoch #84: loss=4.15913200378418\n",
            "Epoch #85: loss=3.594775915145874\n",
            "Epoch #86: loss=4.0483903884887695\n",
            "Epoch #87: loss=4.178118705749512\n",
            "Epoch #88: loss=3.7491960525512695\n",
            "Epoch #89: loss=4.018913745880127\n",
            "Epoch #90: loss=4.297555923461914\n",
            "Epoch #91: loss=3.943845272064209\n",
            "Epoch #92: loss=3.949582576751709\n",
            "Epoch #93: loss=3.984551429748535\n",
            "Epoch #94: loss=3.885077953338623\n",
            "Epoch #95: loss=3.7743566036224365\n",
            "Epoch #96: loss=3.5534262657165527\n",
            "Epoch #97: loss=4.122632026672363\n",
            "Epoch #98: loss=3.9511115550994873\n",
            "Epoch #99: loss=4.381870269775391\n",
            "Epoch #100: loss=4.778227806091309\n",
            "Epoch #101: loss=3.8900420665740967\n",
            "Epoch #102: loss=3.716097116470337\n",
            "Epoch #103: loss=3.3790841102600098\n",
            "Epoch #104: loss=3.699093818664551\n",
            "Epoch #105: loss=4.044259548187256\n",
            "Epoch #106: loss=3.75482177734375\n",
            "Epoch #107: loss=3.5654540061950684\n",
            "Epoch #108: loss=3.713894844055176\n",
            "Epoch #109: loss=3.585564374923706\n",
            "Epoch #110: loss=3.2128825187683105\n",
            "Epoch #111: loss=4.2884202003479\n",
            "Epoch #112: loss=3.6910433769226074\n",
            "Epoch #113: loss=3.930694818496704\n",
            "Epoch #114: loss=3.712272882461548\n",
            "Epoch #115: loss=3.8516857624053955\n",
            "Epoch #116: loss=3.314481019973755\n",
            "Epoch #117: loss=3.5224270820617676\n",
            "Epoch #118: loss=3.969230890274048\n",
            "Epoch #119: loss=3.7942800521850586\n",
            "Epoch #120: loss=3.4908437728881836\n",
            "Epoch #121: loss=4.1627373695373535\n",
            "Epoch #122: loss=3.507687568664551\n",
            "Epoch #123: loss=3.372398853302002\n",
            "Epoch #124: loss=4.1424994468688965\n",
            "Epoch #125: loss=3.660984516143799\n",
            "Epoch #126: loss=3.738750696182251\n",
            "Epoch #127: loss=3.658618927001953\n",
            "Epoch #128: loss=3.5636746883392334\n",
            "Epoch #129: loss=3.0117852687835693\n",
            "Epoch #130: loss=4.2142486572265625\n",
            "Epoch #131: loss=3.702799081802368\n",
            "Epoch #132: loss=3.984788417816162\n",
            "Epoch #133: loss=3.8550238609313965\n",
            "Epoch #134: loss=3.056364059448242\n",
            "Epoch #135: loss=3.6633810997009277\n",
            "Epoch #136: loss=3.5315587520599365\n",
            "Epoch #137: loss=4.0408525466918945\n",
            "Epoch #138: loss=3.1738762855529785\n",
            "Epoch #139: loss=4.119410991668701\n",
            "Epoch #140: loss=3.711104393005371\n",
            "Epoch #141: loss=3.6020100116729736\n",
            "Epoch #142: loss=3.452390432357788\n",
            "Epoch #143: loss=4.116894245147705\n",
            "Epoch #144: loss=3.742975950241089\n",
            "Epoch #145: loss=3.3479037284851074\n",
            "Epoch #146: loss=3.3603429794311523\n",
            "Epoch #147: loss=4.2748236656188965\n",
            "Epoch #148: loss=3.1259260177612305\n",
            "Epoch #149: loss=3.1484718322753906\n",
            "Epoch #150: loss=3.6503958702087402\n",
            "Epoch #151: loss=3.3991336822509766\n",
            "Epoch #152: loss=2.8925373554229736\n",
            "Epoch #153: loss=3.0786678791046143\n",
            "Epoch #154: loss=3.3611273765563965\n",
            "Epoch #155: loss=3.712879180908203\n",
            "Epoch #156: loss=4.470762252807617\n",
            "Epoch #157: loss=2.222028970718384\n",
            "Epoch #158: loss=3.1894357204437256\n",
            "Epoch #159: loss=3.668092727661133\n",
            "Epoch #160: loss=2.969525098800659\n",
            "Epoch #161: loss=3.795203924179077\n",
            "Epoch #162: loss=3.729487895965576\n",
            "Epoch #163: loss=3.1397345066070557\n",
            "Epoch #164: loss=3.5281646251678467\n",
            "Epoch #165: loss=2.871433973312378\n",
            "Epoch #166: loss=3.2681162357330322\n",
            "Epoch #167: loss=3.436854362487793\n",
            "Epoch #168: loss=3.6561360359191895\n",
            "Epoch #169: loss=3.575819492340088\n",
            "Epoch #170: loss=2.9119973182678223\n",
            "Epoch #171: loss=3.1322646141052246\n",
            "Epoch #172: loss=3.8768680095672607\n",
            "Epoch #173: loss=3.733389139175415\n",
            "Epoch #174: loss=3.798304319381714\n",
            "Epoch #175: loss=3.655649423599243\n",
            "Epoch #176: loss=2.8192927837371826\n",
            "Epoch #177: loss=3.8407223224639893\n",
            "Epoch #178: loss=2.8754947185516357\n",
            "Epoch #179: loss=3.528088092803955\n",
            "Epoch #180: loss=3.94364595413208\n",
            "Epoch #181: loss=3.782930612564087\n",
            "Epoch #182: loss=3.120772361755371\n",
            "Epoch #183: loss=3.0884013175964355\n",
            "Epoch #184: loss=2.928978443145752\n",
            "Epoch #185: loss=2.8904874324798584\n",
            "Epoch #186: loss=3.783916473388672\n",
            "Epoch #187: loss=3.2127251625061035\n",
            "Epoch #188: loss=2.759256362915039\n",
            "Epoch #189: loss=3.641105890274048\n",
            "Epoch #190: loss=3.5952248573303223\n",
            "Epoch #191: loss=2.9337148666381836\n",
            "Epoch #192: loss=3.8857593536376953\n",
            "Epoch #193: loss=3.9091274738311768\n",
            "Epoch #194: loss=3.2106637954711914\n",
            "Epoch #195: loss=4.1829986572265625\n",
            "Epoch #196: loss=3.5005338191986084\n",
            "Epoch #197: loss=3.523191452026367\n",
            "Epoch #198: loss=3.7419488430023193\n",
            "Epoch #199: loss=2.579486608505249\n",
            "Epoch #200: loss=4.030109882354736\n",
            "Epoch #201: loss=2.398350954055786\n",
            "Epoch #202: loss=3.8072946071624756\n",
            "Epoch #203: loss=3.4345874786376953\n",
            "Epoch #204: loss=3.9410736560821533\n",
            "Epoch #205: loss=2.4749197959899902\n",
            "Epoch #206: loss=3.1550137996673584\n",
            "Epoch #207: loss=2.6729750633239746\n",
            "Epoch #208: loss=2.002816915512085\n",
            "Epoch #209: loss=3.773991584777832\n",
            "Epoch #210: loss=3.8367505073547363\n",
            "Epoch #211: loss=4.065218448638916\n",
            "Epoch #212: loss=2.4000473022460938\n",
            "Epoch #213: loss=2.4074201583862305\n",
            "Epoch #214: loss=3.4239869117736816\n",
            "Epoch #215: loss=4.003596782684326\n",
            "Epoch #216: loss=2.932162046432495\n",
            "Epoch #217: loss=2.0479087829589844\n",
            "Epoch #218: loss=2.9549875259399414\n",
            "Epoch #219: loss=3.7163326740264893\n",
            "Epoch #220: loss=3.7599704265594482\n",
            "Epoch #221: loss=2.5852456092834473\n",
            "Epoch #222: loss=2.461965799331665\n",
            "Epoch #223: loss=2.9444196224212646\n",
            "Epoch #224: loss=3.3038382530212402\n",
            "Epoch #225: loss=3.243590831756592\n",
            "Epoch #226: loss=3.1470181941986084\n",
            "Epoch #227: loss=2.666724443435669\n",
            "Epoch #228: loss=2.9509048461914062\n",
            "Epoch #229: loss=2.728423833847046\n",
            "Epoch #230: loss=2.749908924102783\n",
            "Epoch #231: loss=3.3987464904785156\n",
            "Epoch #232: loss=4.029514789581299\n",
            "Epoch #233: loss=4.069245338439941\n",
            "Epoch #234: loss=4.150454044342041\n",
            "Epoch #235: loss=2.9069974422454834\n",
            "Epoch #236: loss=2.9908862113952637\n",
            "Epoch #237: loss=3.82078218460083\n",
            "Epoch #238: loss=3.2424776554107666\n",
            "Epoch #239: loss=3.9165456295013428\n",
            "Epoch #240: loss=3.5276598930358887\n",
            "Epoch #241: loss=3.8687632083892822\n",
            "Epoch #242: loss=3.4300789833068848\n",
            "Epoch #243: loss=3.546797275543213\n",
            "Epoch #244: loss=3.6235737800598145\n",
            "Epoch #245: loss=2.4621081352233887\n",
            "Epoch #246: loss=3.540705919265747\n",
            "Epoch #247: loss=3.0106444358825684\n",
            "Epoch #248: loss=3.2297778129577637\n",
            "Epoch #249: loss=3.340768814086914\n",
            "Epoch #250: loss=2.877842664718628\n",
            "Epoch #251: loss=3.6345295906066895\n",
            "Epoch #252: loss=2.956378698348999\n",
            "Epoch #253: loss=2.637397527694702\n",
            "Epoch #254: loss=3.152789354324341\n",
            "Epoch #255: loss=2.429002285003662\n",
            "Epoch #256: loss=2.7557284832000732\n",
            "Epoch #257: loss=3.1530215740203857\n",
            "Epoch #258: loss=3.5633468627929688\n",
            "Epoch #259: loss=3.2000319957733154\n",
            "Epoch #260: loss=3.6406972408294678\n",
            "Epoch #261: loss=2.145679235458374\n",
            "Epoch #262: loss=3.2531769275665283\n",
            "Epoch #263: loss=3.5700366497039795\n",
            "Epoch #264: loss=2.679454803466797\n",
            "Epoch #265: loss=3.4561707973480225\n",
            "Epoch #266: loss=3.1458418369293213\n",
            "Epoch #267: loss=2.5470454692840576\n",
            "Epoch #268: loss=2.701631546020508\n",
            "Epoch #269: loss=2.9833881855010986\n",
            "Epoch #270: loss=3.10081148147583\n",
            "Epoch #271: loss=2.7205116748809814\n",
            "Epoch #272: loss=3.2695462703704834\n",
            "Epoch #273: loss=4.4668426513671875\n",
            "Epoch #274: loss=3.7026925086975098\n",
            "Epoch #275: loss=3.2847750186920166\n",
            "Epoch #276: loss=2.9594626426696777\n",
            "Epoch #277: loss=3.0151989459991455\n",
            "Epoch #278: loss=1.9351234436035156\n",
            "Epoch #279: loss=2.92889142036438\n",
            "Epoch #280: loss=3.653757333755493\n",
            "Epoch #281: loss=2.428492546081543\n",
            "Epoch #282: loss=3.2852325439453125\n",
            "Epoch #283: loss=2.690887689590454\n",
            "Epoch #284: loss=3.2395260334014893\n",
            "Epoch #285: loss=1.93595552444458\n",
            "Epoch #286: loss=3.4514400959014893\n",
            "Epoch #287: loss=1.9180938005447388\n",
            "Epoch #288: loss=2.222403049468994\n",
            "Epoch #289: loss=3.5557470321655273\n",
            "Epoch #290: loss=2.850886106491089\n",
            "Epoch #291: loss=2.62255597114563\n",
            "Epoch #292: loss=3.351280450820923\n",
            "Epoch #293: loss=2.8290340900421143\n",
            "Epoch #294: loss=3.675197124481201\n",
            "Epoch #295: loss=3.0013649463653564\n",
            "Epoch #296: loss=3.97021746635437\n",
            "Epoch #297: loss=4.393706798553467\n",
            "Epoch #298: loss=3.1476831436157227\n",
            "Epoch #299: loss=2.7324554920196533\n",
            "\n",
            "Training time: 0:08:18.094768\n",
            "\n",
            "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.38628846496531544, 'MAE': 0.43036223013201613}, 'raw': {'MSE': 0.026034013728258646, 'MAE': 0.11172460903997031}}, 48: {'norm': {'MSE': 0.882263816398366, 'MAE': 0.6703939151758738}, 'raw': {'MSE': 0.05946040304321081, 'MAE': 0.17403826860330934}}, 96: {'norm': {'MSE': 1.0806033012707663, 'MAE': 0.7871344716676034}, 'raw': {'MSE': 0.07282754563202003, 'MAE': 0.20434481611872712}}, 168: {'norm': {'MSE': 0.7102902308071766, 'MAE': 0.5981464485745398}, 'raw': {'MSE': 0.04787019855291313, 'MAE': 0.15528239574020383}}, 336: {'norm': {'MSE': 0.26951494568782447, 'MAE': 0.3815407531028477}, 'raw': {'MSE': 0.018164031267209, 'MAE': 0.0990502611910946}}}, 'encoder_infer_time': 3.8076155185699463, 'lr_train_time': {24: 0.24136805534362793, 48: 0.21942138671875, 96: 0.30964016914367676, 168: 0.3712315559387207, 336: 0.4928734302520752}, 'lr_infer_time': {24: 0.0006866455078125, 48: 0.0008254051208496094, 96: 0.0018770694732666016, 168: 0.003350973129272461, 336: 0.0035364627838134766}}\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For mendeley :-\n",
        "  + Rename 'Time' column to 'data'\n",
        "  + Remove 678 in datautils.py line 84\n",
        "  + Add data = data[['Voltage']] in datautils.py line 47\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dQfHhJD7ToZn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c542002-c0a0-4474-e899-d7566750521a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFor mendeley :-\\n  + Rename 'Time' column to 'data'\\n  + Remove 678 in datautils.py line 84\\n  + Add data = data[['Voltage']] in datautils.py line 47\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ikvz5ihwI9h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}